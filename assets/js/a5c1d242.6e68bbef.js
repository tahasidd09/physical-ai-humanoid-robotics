"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[225],{6375:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"module2/week7-gazebo-unity","title":"Unity Integration and Sensor Simulation","description":"High-fidelity rendering with Unity and simulating LiDAR, depth cameras, and IMUs","source":"@site/docs/module2/week7-gazebo-unity.md","sourceDirName":"module2","slug":"/module2/week7-gazebo-unity","permalink":"/physical-ai-and-humanoid-robotics/docs/module2/week7-gazebo-unity","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Unity Integration and Sensor Simulation","description":"High-fidelity rendering with Unity and simulating LiDAR, depth cameras, and IMUs","keywords":["unity","sensors","lidar","depth-camera","imu","simulation"]},"sidebar":"tutorialSidebar","previous":{"title":"Robot Simulation with Gazebo","permalink":"/physical-ai-and-humanoid-robotics/docs/module2/week6-gazebo"},"next":{"title":"Module 3: NVIDIA Isaac","permalink":"/physical-ai-and-humanoid-robotics/docs/category/module-3-nvidia-isaac"}}');var a=i(4848),t=i(8453);const r={sidebar_position:2,title:"Unity Integration and Sensor Simulation",description:"High-fidelity rendering with Unity and simulating LiDAR, depth cameras, and IMUs",keywords:["unity","sensors","lidar","depth-camera","imu","simulation"]},o="Week 7: Unity Integration and Sensor Simulation",l={},d=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"The Physics (Why)",id:"the-physics-why",level:2},{value:"The Analogy (Mental Model)",id:"the-analogy-mental-model",level:2},{value:"The Visualization (Sensor Pipeline)",id:"the-visualization-sensor-pipeline",level:2},{value:"The Code (Implementation)",id:"the-code-implementation",level:2},{value:"LiDAR Sensor Configuration (SDF)",id:"lidar-sensor-configuration-sdf",level:3},{value:"Depth Camera Configuration",id:"depth-camera-configuration",level:3},{value:"IMU Configuration",id:"imu-configuration",level:3},{value:"Processing Sensor Data in ROS 2",id:"processing-sensor-data-in-ros-2",level:3},{value:"The Hardware Reality (Warning)",id:"the-hardware-reality-warning",level:2},{value:"Assessment",id:"assessment",level:2},{value:"Recall",id:"recall",level:3},{value:"Apply",id:"apply",level:3},{value:"Analyze",id:"analyze",level:3}];function c(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"week-7-unity-integration-and-sensor-simulation",children:"Week 7: Unity Integration and Sensor Simulation"})}),"\n",(0,a.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this chapter, you should be able to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Understand when to use Unity vs. Gazebo for robot simulation"}),"\n",(0,a.jsx)(n.li,{children:"Configure LiDAR, depth camera, and IMU sensors in simulation"}),"\n",(0,a.jsx)(n.li,{children:"Process simulated sensor data in ROS 2"}),"\n",(0,a.jsx)(n.li,{children:"Create realistic sensor noise models"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"the-physics-why",children:"The Physics (Why)"}),"\n",(0,a.jsxs)(n.p,{children:["While Gazebo excels at physics simulation, ",(0,a.jsx)(n.strong,{children:"Unity"})," provides superior visual fidelity\u2014essential for training computer vision models. Additionally, accurate ",(0,a.jsx)(n.strong,{children:"sensor simulation"})," is critical because robots perceive the world through sensors, not direct access to simulation state."]}),"\n",(0,a.jsx)(n.h2,{id:"the-analogy-mental-model",children:"The Analogy (Mental Model)"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Simulator"}),(0,a.jsx)(n.th,{children:"Strength"}),(0,a.jsx)(n.th,{children:"Best For"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Gazebo"})}),(0,a.jsx)(n.td,{children:"Physics accuracy"}),(0,a.jsx)(n.td,{children:"Control algorithms, dynamics"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Unity"})}),(0,a.jsx)(n.td,{children:"Visual realism"}),(0,a.jsx)(n.td,{children:"Vision AI, human interaction"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Isaac Sim"})}),(0,a.jsx)(n.td,{children:"Both + GPU"}),(0,a.jsx)(n.td,{children:"Production sim-to-real"})]})]})]}),"\n",(0,a.jsx)(n.p,{children:"Think of it like movie production: Gazebo is the stunt coordinator (physics), Unity is the cinematographer (visuals)."}),"\n",(0,a.jsx)(n.h2,{id:"the-visualization-sensor-pipeline",children:"The Visualization (Sensor Pipeline)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-mermaid",children:'graph LR\n    subgraph "Physical World"\n        A[Robot] --\x3e B[Environment]\n    end\n    \n    subgraph "Sensor Simulation"\n        B --\x3e C[LiDAR Plugin]\n        B --\x3e D[Camera Plugin]\n        B --\x3e E[IMU Plugin]\n    end\n    \n    subgraph "ROS 2 Topics"\n        C --\x3e F[/scan]\n        D --\x3e G[/camera/image]\n        D --\x3e H[/camera/depth]\n        E --\x3e I[/imu]\n    end\n    \n    subgraph "Perception Stack"\n        F --\x3e J[SLAM]\n        G --\x3e K[Object Detection]\n        H --\x3e J\n        I --\x3e L[State Estimation]\n    end\n'})}),"\n",(0,a.jsx)(n.h2,{id:"the-code-implementation",children:"The Code (Implementation)"}),"\n",(0,a.jsx)(n.h3,{id:"lidar-sensor-configuration-sdf",children:"LiDAR Sensor Configuration (SDF)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'\x3c!-- lidar_sensor.sdf - 2D LiDAR configuration --\x3e\n<sensor name="lidar" type="gpu_lidar">\n  <pose>0 0 0.1 0 0 0</pose>\n  <topic>/scan</topic>\n  <update_rate>10</update_rate>\n  <lidar>\n    <scan>\n      <horizontal>\n        <samples>360</samples>\n        <resolution>1</resolution>\n        <min_angle>-3.14159</min_angle>\n        <max_angle>3.14159</max_angle>\n      </horizontal>\n    </scan>\n    <range>\n      <min>0.1</min>\n      <max>10.0</max>\n      <resolution>0.01</resolution>\n    </range>\n    <noise>\n      <type>gaussian</type>\n      <mean>0.0</mean>\n      <stddev>0.01</stddev>\n    </noise>\n  </lidar>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"depth-camera-configuration",children:"Depth Camera Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'\x3c!-- depth_camera.sdf - Intel RealSense D435i style --\x3e\n<sensor name="depth_camera" type="depth_camera">\n  <pose>0.1 0 0.5 0 0 0</pose>\n  <update_rate>30</update_rate>\n  <camera>\n    <horizontal_fov>1.047</horizontal_fov>\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R_FLOAT32</format>\n    </image>\n    <clip>\n      <near>0.1</near>\n      <far>10.0</far>\n    </clip>\n    <noise>\n      <type>gaussian</type>\n      <mean>0.0</mean>\n      <stddev>0.005</stddev>\n    </noise>\n  </camera>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"imu-configuration",children:"IMU Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'\x3c!-- imu_sensor.sdf - 6-axis IMU --\x3e\n<sensor name="imu" type="imu">\n  <pose>0 0 0.3 0 0 0</pose>\n  <update_rate>100</update_rate>\n  <imu>\n    <angular_velocity>\n      <x><noise type="gaussian"><mean>0</mean><stddev>0.001</stddev></noise></x>\n      <y><noise type="gaussian"><mean>0</mean><stddev>0.001</stddev></noise></y>\n      <z><noise type="gaussian"><mean>0</mean><stddev>0.001</stddev></noise></z>\n    </angular_velocity>\n    <linear_acceleration>\n      <x><noise type="gaussian"><mean>0</mean><stddev>0.01</stddev></noise></x>\n      <y><noise type="gaussian"><mean>0</mean><stddev>0.01</stddev></noise></y>\n      <z><noise type="gaussian"><mean>0</mean><stddev>0.01</stddev></noise></z>\n    </linear_acceleration>\n  </imu>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"processing-sensor-data-in-ros-2",children:"Processing Sensor Data in ROS 2"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nsensor_processor.py - Process simulated sensor data.\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Image, Imu\nfrom cv_bridge import CvBridge\nimport numpy as np\n\n\nclass SensorProcessor(Node):\n    """Processes data from simulated sensors."""\n    \n    def __init__(self):\n        super().__init__(\'sensor_processor\')\n        \n        self.bridge = CvBridge()\n        \n        # LiDAR subscriber\n        self.lidar_sub = self.create_subscription(\n            LaserScan, \'/scan\', self.lidar_callback, 10\n        )\n        \n        # Depth camera subscriber\n        self.depth_sub = self.create_subscription(\n            Image, \'/camera/depth\', self.depth_callback, 10\n        )\n        \n        # IMU subscriber\n        self.imu_sub = self.create_subscription(\n            Imu, \'/imu\', self.imu_callback, 10\n        )\n        \n        self.get_logger().info(\'Sensor processor started\')\n    \n    def lidar_callback(self, msg: LaserScan):\n        """Process LiDAR scan for obstacle detection."""\n        ranges = np.array(msg.ranges)\n        \n        # Find minimum distance (closest obstacle)\n        valid_ranges = ranges[np.isfinite(ranges)]\n        if len(valid_ranges) > 0:\n            min_dist = np.min(valid_ranges)\n            min_idx = np.argmin(valid_ranges)\n            angle = msg.angle_min + min_idx * msg.angle_increment\n            \n            if min_dist < 0.5:\n                self.get_logger().warn(\n                    f\'Obstacle at {min_dist:.2f}m, angle {np.degrees(angle):.1f}\xb0\'\n                )\n    \n    def depth_callback(self, msg: Image):\n        """Process depth image for 3D perception."""\n        depth_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'32FC1\')\n        \n        # Calculate average depth in center region\n        h, w = depth_image.shape\n        center = depth_image[h//3:2*h//3, w//3:2*w//3]\n        avg_depth = np.nanmean(center)\n        \n        self.get_logger().debug(f\'Center depth: {avg_depth:.2f}m\')\n    \n    def imu_callback(self, msg: Imu):\n        """Process IMU for orientation estimation."""\n        # Extract orientation (quaternion)\n        q = msg.orientation\n        \n        # Extract angular velocity\n        omega = msg.angular_velocity\n        \n        # Extract linear acceleration\n        accel = msg.linear_acceleration\n        \n        # Simple tilt detection from accelerometer\n        tilt_x = np.arctan2(accel.y, accel.z)\n        tilt_y = np.arctan2(-accel.x, np.sqrt(accel.y**2 + accel.z**2))\n        \n        if abs(tilt_x) > 0.3 or abs(tilt_y) > 0.3:\n            self.get_logger().warn(\n                f\'High tilt detected: roll={np.degrees(tilt_x):.1f}\xb0, \'\n                f\'pitch={np.degrees(tilt_y):.1f}\xb0\'\n            )\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SensorProcessor()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"the-hardware-reality-warning",children:"The Hardware Reality (Warning)"}),"\n",(0,a.jsxs)(n.admonition,{title:"Sensor Noise is Critical",type:"danger",children:[(0,a.jsx)(n.p,{children:"Simulated sensors without noise produce unrealistically clean data. Always add realistic noise models:"}),(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"LiDAR"}),": Gaussian noise + occasional dropouts"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Camera"}),": Motion blur, exposure variation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"IMU"}),": Bias drift, temperature effects"]}),"\n"]})]}),"\n",(0,a.jsx)(n.admonition,{title:"Unity for Vision AI",type:"tip",children:(0,a.jsx)(n.p,{children:"If training neural networks for object detection or segmentation, Unity's photorealistic rendering produces better training data than Gazebo's basic graphics."})}),"\n",(0,a.jsx)(n.h2,{id:"assessment",children:"Assessment"}),"\n",(0,a.jsx)(n.h3,{id:"recall",children:"Recall"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"What are the key differences between Gazebo and Unity for robotics?"}),"\n",(0,a.jsx)(n.li,{children:"What noise parameters should be configured for a simulated IMU?"}),"\n",(0,a.jsx)(n.li,{children:"How do you process depth images in ROS 2?"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"apply",children:"Apply"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Configure a 3D LiDAR sensor with 16 vertical beams and realistic noise."}),"\n",(0,a.jsx)(n.li,{children:"Write a node that fuses LiDAR and depth camera data for obstacle detection."}),"\n",(0,a.jsx)(n.li,{children:"Create a sensor noise model that varies based on distance (more noise at longer ranges)."}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"analyze",children:"Analyze"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Why might a vision algorithm trained on Unity data fail on real camera images?"}),"\n",(0,a.jsx)(n.li,{children:"Compare the computational cost of simulating LiDAR vs. depth cameras."}),"\n",(0,a.jsx)(n.li,{children:"Design a sensor suite for a humanoid robot that must navigate outdoors."}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var s=i(6540);const a={},t=s.createContext(a);function r(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);