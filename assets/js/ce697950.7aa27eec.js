"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[75],{3665:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"module3/week10-isaac-3","title":"Sim-to-Real Transfer","description":"Bridging the reality gap - domain randomization, reinforcement learning, and deploying simulation-trained models to real robots","source":"@site/docs/module3/week10-isaac-3.md","sourceDirName":"module3","slug":"/module3/week10-isaac-3","permalink":"/physical-ai-and-humanoid-robotics/docs/module3/week10-isaac-3","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Sim-to-Real Transfer","description":"Bridging the reality gap - domain randomization, reinforcement learning, and deploying simulation-trained models to real robots","keywords":["sim-to-real","domain-randomization","reinforcement-learning","transfer-learning"]},"sidebar":"tutorialSidebar","previous":{"title":"Isaac ROS and VSLAM","permalink":"/physical-ai-and-humanoid-robotics/docs/module3/week9-isaac-2"},"next":{"title":"Module 4: Vision-Language-Action","permalink":"/physical-ai-and-humanoid-robotics/docs/category/module-4-vision-language-action"}}');var o=i(4848),t=i(8453);const r={sidebar_position:3,title:"Sim-to-Real Transfer",description:"Bridging the reality gap - domain randomization, reinforcement learning, and deploying simulation-trained models to real robots",keywords:["sim-to-real","domain-randomization","reinforcement-learning","transfer-learning"]},s="Sim-to-Real Transfer",l={},d=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"The Physics (Why)",id:"the-physics-why",level:2},{value:"The Analogy (Mental Model)",id:"the-analogy-mental-model",level:2},{value:"The Visualization (Sim-to-Real Pipeline)",id:"the-visualization-sim-to-real-pipeline",level:2},{value:"The Code (Implementation)",id:"the-code-implementation",level:2},{value:"Domain Randomization",id:"domain-randomization",level:3},{value:"Reinforcement Learning Training",id:"reinforcement-learning-training",level:3},{value:"Deployment to Real Robot",id:"deployment-to-real-robot",level:3},{value:"The Hardware Reality (Warning)",id:"the-hardware-reality-warning",level:2},{value:"Debugging Sim-to-Real Gaps",id:"debugging-sim-to-real-gaps",level:3},{value:"Assessment",id:"assessment",level:2},{value:"Recall",id:"recall",level:3},{value:"Apply",id:"apply",level:3},{value:"Analyze",id:"analyze",level:3}];function c(n){const e={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"sim-to-real-transfer",children:"Sim-to-Real Transfer"})}),"\n",(0,o.jsx)(e.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,o.jsx)(e.p,{children:"By the end of this chapter, you should be able to:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Understand the reality gap and why sim-to-real transfer is challenging"}),"\n",(0,o.jsx)(e.li,{children:"Implement domain randomization to improve model robustness"}),"\n",(0,o.jsx)(e.li,{children:"Train reinforcement learning policies in simulation"}),"\n",(0,o.jsx)(e.li,{children:"Deploy simulation-trained models to real robots"}),"\n",(0,o.jsx)(e.li,{children:"Evaluate and debug sim-to-real transfer failures"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"the-physics-why",children:"The Physics (Why)"}),"\n",(0,o.jsxs)(e.p,{children:["The ",(0,o.jsx)(e.strong,{children:"reality gap"})," is the difference between simulated and real-world physics. Simulators make approximations:"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Friction"}),": Real friction is complex (static vs. dynamic, surface variations)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Contact dynamics"}),": Real collisions involve deformation, vibration"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Actuator dynamics"}),": Real motors have delays, backlash, temperature effects"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Sensor noise"}),": Real sensors have noise patterns simulators can't perfectly model"]}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"A policy trained in simulation may fail catastrophically on a real robot because it learned to exploit simulator quirks rather than robust physical principles."}),"\n",(0,o.jsx)(e.h2,{id:"the-analogy-mental-model",children:"The Analogy (Mental Model)"}),"\n",(0,o.jsxs)(e.p,{children:["Think of sim-to-real transfer like ",(0,o.jsx)(e.strong,{children:"learning to drive in a video game, then driving a real car"}),":"]}),"\n",(0,o.jsxs)(e.table,{children:[(0,o.jsx)(e.thead,{children:(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.th,{children:"Video Game"}),(0,o.jsx)(e.th,{children:"Real World"})]})}),(0,o.jsxs)(e.tbody,{children:[(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"Perfect traction"}),(0,o.jsx)(e.td,{children:"Ice, gravel, wet roads"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"Instant response"}),(0,o.jsx)(e.td,{children:"Steering delay, brake fade"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"No consequences"}),(0,o.jsx)(e.td,{children:"Accidents cause damage"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"Reset on failure"}),(0,o.jsx)(e.td,{children:"No respawns"})]})]})]}),"\n",(0,o.jsx)(e.p,{children:"To succeed, you need to:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Train with variety"}),": Different weather, road conditions"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Add noise"}),": Imperfect controls, sensor lag"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Be conservative"}),": Don't rely on perfect physics"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"the-visualization-sim-to-real-pipeline",children:"The Visualization (Sim-to-Real Pipeline)"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-mermaid",children:'graph TB\n    subgraph "Simulation Training"\n        A[Isaac Sim] --\x3e B[Domain Randomization]\n        B --\x3e C[RL Training]\n        C --\x3e D[Policy Network]\n    end\n    \n    subgraph "Transfer Techniques"\n        D --\x3e E[System Identification]\n        D --\x3e F[Fine-tuning]\n        D --\x3e G[Residual Learning]\n    end\n    \n    subgraph "Real Robot"\n        E --\x3e H[Calibrated Sim]\n        F --\x3e I[Real Data]\n        G --\x3e J[Hybrid Policy]\n        H --\x3e K[Deployment]\n        I --\x3e K\n        J --\x3e K\n    end\n    \n    K --\x3e L[Performance Evaluation]\n    L --\x3e|Gap detected| B\n'})}),"\n",(0,o.jsx)(e.h2,{id:"the-code-implementation",children:"The Code (Implementation)"}),"\n",(0,o.jsx)(e.h3,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\ndomain_randomization.py - Randomize simulation parameters for robust training.\n"""\n\nimport numpy as np\nfrom dataclasses import dataclass\nfrom typing import Dict, Tuple\nimport random\n\n\n@dataclass\nclass RandomizationConfig:\n    """Configuration for domain randomization."""\n    # Physics parameters\n    friction_range: Tuple[float, float] = (0.5, 1.5)\n    mass_scale_range: Tuple[float, float] = (0.8, 1.2)\n    motor_strength_range: Tuple[float, float] = (0.9, 1.1)\n    \n    # Sensor noise\n    observation_noise_std: float = 0.01\n    action_delay_range: Tuple[int, int] = (0, 3)  # frames\n    \n    # Visual randomization\n    lighting_range: Tuple[float, float] = (0.5, 1.5)\n    texture_randomization: bool = True\n\n\nclass DomainRandomizer:\n    """Apply domain randomization to simulation."""\n    \n    def __init__(self, config: RandomizationConfig):\n        self.config = config\n        self.current_params = {}\n    \n    def randomize_physics(self) -> Dict[str, float]:\n        """Randomize physics parameters."""\n        params = {\n            \'friction\': np.random.uniform(*self.config.friction_range),\n            \'mass_scale\': np.random.uniform(*self.config.mass_scale_range),\n            \'motor_strength\': np.random.uniform(*self.config.motor_strength_range),\n        }\n        self.current_params.update(params)\n        return params\n    \n    def add_observation_noise(self, obs: np.ndarray) -> np.ndarray:\n        """Add noise to observations."""\n        noise = np.random.normal(0, self.config.observation_noise_std, obs.shape)\n        return obs + noise\n    \n    def apply_action_delay(self, action: np.ndarray, action_buffer: list) -> np.ndarray:\n        """Apply random action delay."""\n        delay = np.random.randint(*self.config.action_delay_range)\n        action_buffer.append(action)\n        \n        if len(action_buffer) > delay:\n            return action_buffer.pop(0)\n        return np.zeros_like(action)\n    \n    def randomize_visual(self) -> Dict[str, float]:\n        """Randomize visual parameters."""\n        params = {\n            \'lighting_intensity\': np.random.uniform(*self.config.lighting_range),\n            \'texture_id\': random.randint(0, 100) if self.config.texture_randomization else 0,\n        }\n        self.current_params.update(params)\n        return params\n\n\nclass RobustTrainingEnv:\n    """Training environment with domain randomization."""\n    \n    def __init__(self, base_env, randomizer: DomainRandomizer):\n        self.base_env = base_env\n        self.randomizer = randomizer\n        self.action_buffer = []\n        self.episode_count = 0\n    \n    def reset(self):\n        """Reset environment with new randomization."""\n        self.episode_count += 1\n        self.action_buffer = []\n        \n        # Randomize at start of each episode\n        physics_params = self.randomizer.randomize_physics()\n        visual_params = self.randomizer.randomize_visual()\n        \n        # Apply to simulation (implementation depends on simulator)\n        self.apply_randomization(physics_params, visual_params)\n        \n        obs = self.base_env.reset()\n        return self.randomizer.add_observation_noise(obs)\n    \n    def step(self, action):\n        """Step with randomization applied."""\n        # Apply action delay\n        delayed_action = self.randomizer.apply_action_delay(\n            action, self.action_buffer\n        )\n        \n        obs, reward, done, info = self.base_env.step(delayed_action)\n        \n        # Add observation noise\n        noisy_obs = self.randomizer.add_observation_noise(obs)\n        \n        return noisy_obs, reward, done, info\n    \n    def apply_randomization(self, physics_params, visual_params):\n        """Apply randomization to simulator."""\n        # Implementation depends on specific simulator\n        pass\n'})}),"\n",(0,o.jsx)(e.h3,{id:"reinforcement-learning-training",children:"Reinforcement Learning Training"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nrl_training.py - Train walking policy with PPO.\n"""\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom typing import Tuple\n\n\nclass WalkingPolicy(nn.Module):\n    """Neural network policy for humanoid walking."""\n    \n    def __init__(self, obs_dim: int, action_dim: int, hidden_dim: int = 256):\n        super().__init__()\n        \n        # Actor network (policy)\n        self.actor = nn.Sequential(\n            nn.Linear(obs_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim),\n            nn.Tanh()  # Actions in [-1, 1]\n        )\n        \n        # Critic network (value function)\n        self.critic = nn.Sequential(\n            nn.Linear(obs_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n        \n        # Log standard deviation for action distribution\n        self.log_std = nn.Parameter(torch.zeros(action_dim))\n    \n    def forward(self, obs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        """Get action mean and value estimate."""\n        action_mean = self.actor(obs)\n        value = self.critic(obs)\n        return action_mean, value\n    \n    def get_action(self, obs: torch.Tensor, deterministic: bool = False):\n        """Sample action from policy."""\n        action_mean, _ = self.forward(obs)\n        \n        if deterministic:\n            return action_mean\n        \n        std = torch.exp(self.log_std)\n        dist = torch.distributions.Normal(action_mean, std)\n        action = dist.sample()\n        \n        return action, dist.log_prob(action).sum(-1)\n\n\nclass WalkingReward:\n    """Reward function for humanoid walking."""\n    \n    def __init__(self):\n        # Reward weights\n        self.forward_weight = 1.0\n        self.alive_bonus = 0.1\n        self.energy_penalty = 0.01\n        self.stability_weight = 0.5\n    \n    def compute(\n        self,\n        velocity: np.ndarray,\n        joint_torques: np.ndarray,\n        orientation: np.ndarray,\n        is_fallen: bool\n    ) -> float:\n        """Compute reward for current state."""\n        if is_fallen:\n            return -10.0  # Large penalty for falling\n        \n        # Forward velocity reward\n        forward_reward = self.forward_weight * velocity[0]\n        \n        # Alive bonus\n        alive_reward = self.alive_bonus\n        \n        # Energy penalty (encourage efficient movement)\n        energy_penalty = self.energy_penalty * np.sum(joint_torques ** 2)\n        \n        # Stability reward (upright orientation)\n        # orientation is roll, pitch, yaw\n        stability_reward = self.stability_weight * (\n            1.0 - abs(orientation[0]) - abs(orientation[1])\n        )\n        \n        total_reward = forward_reward + alive_reward - energy_penalty + stability_reward\n        \n        return total_reward\n\n\n# Training loop (simplified)\ndef train_walking_policy(env, policy, num_episodes=1000):\n    """Train walking policy with PPO."""\n    optimizer = torch.optim.Adam(policy.parameters(), lr=3e-4)\n    reward_fn = WalkingReward()\n    \n    for episode in range(num_episodes):\n        obs = env.reset()\n        episode_reward = 0\n        \n        while True:\n            obs_tensor = torch.FloatTensor(obs).unsqueeze(0)\n            action, log_prob = policy.get_action(obs_tensor)\n            \n            next_obs, _, done, info = env.step(action.numpy().flatten())\n            \n            # Compute custom reward\n            reward = reward_fn.compute(\n                velocity=info.get(\'velocity\', np.zeros(3)),\n                joint_torques=info.get(\'torques\', np.zeros(12)),\n                orientation=info.get(\'orientation\', np.zeros(3)),\n                is_fallen=info.get(\'fallen\', False)\n            )\n            \n            episode_reward += reward\n            obs = next_obs\n            \n            if done:\n                break\n        \n        if episode % 100 == 0:\n            print(f"Episode {episode}: Reward = {episode_reward:.2f}")\n'})}),"\n",(0,o.jsx)(e.h3,{id:"deployment-to-real-robot",children:"Deployment to Real Robot"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nreal_robot_deployment.py - Deploy trained policy to real robot.\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState, Imu\nfrom std_msgs.msg import Float64MultiArray\nimport torch\nimport numpy as np\n\n\nclass RealRobotController(Node):\n    """Deploy simulation-trained policy to real robot."""\n    \n    def __init__(self, policy_path: str):\n        super().__init__(\'real_robot_controller\')\n        \n        # Load trained policy\n        self.policy = torch.load(policy_path)\n        self.policy.eval()\n        \n        # State buffers\n        self.joint_positions = np.zeros(12)\n        self.joint_velocities = np.zeros(12)\n        self.imu_orientation = np.zeros(4)\n        self.imu_angular_vel = np.zeros(3)\n        \n        # Subscribers\n        self.joint_sub = self.create_subscription(\n            JointState, \'/joint_states\', self.joint_callback, 10\n        )\n        self.imu_sub = self.create_subscription(\n            Imu, \'/imu\', self.imu_callback, 10\n        )\n        \n        # Publisher for joint commands\n        self.cmd_pub = self.create_publisher(\n            Float64MultiArray, \'/joint_commands\', 10\n        )\n        \n        # Control loop at 100 Hz\n        self.timer = self.create_timer(0.01, self.control_loop)\n        \n        # Safety limits\n        self.max_joint_velocity = 2.0  # rad/s\n        self.max_torque = 50.0  # Nm\n        \n        self.get_logger().info(\'Real robot controller started\')\n    \n    def joint_callback(self, msg: JointState):\n        """Update joint state from robot."""\n        self.joint_positions = np.array(msg.position[:12])\n        self.joint_velocities = np.array(msg.velocity[:12])\n    \n    def imu_callback(self, msg: Imu):\n        """Update IMU state from robot."""\n        self.imu_orientation = np.array([\n            msg.orientation.x, msg.orientation.y,\n            msg.orientation.z, msg.orientation.w\n        ])\n        self.imu_angular_vel = np.array([\n            msg.angular_velocity.x,\n            msg.angular_velocity.y,\n            msg.angular_velocity.z\n        ])\n    \n    def build_observation(self) -> np.ndarray:\n        """Build observation vector matching training."""\n        obs = np.concatenate([\n            self.joint_positions,\n            self.joint_velocities,\n            self.imu_orientation,\n            self.imu_angular_vel\n        ])\n        return obs\n    \n    def control_loop(self):\n        """Main control loop."""\n        # Build observation\n        obs = self.build_observation()\n        obs_tensor = torch.FloatTensor(obs).unsqueeze(0)\n        \n        # Get action from policy\n        with torch.no_grad():\n            action = self.policy.get_action(obs_tensor, deterministic=True)\n        \n        action = action.numpy().flatten()\n        \n        # Apply safety limits\n        action = self.apply_safety_limits(action)\n        \n        # Publish command\n        cmd_msg = Float64MultiArray()\n        cmd_msg.data = action.tolist()\n        self.cmd_pub.publish(cmd_msg)\n    \n    def apply_safety_limits(self, action: np.ndarray) -> np.ndarray:\n        """Apply safety limits to actions."""\n        # Clip to safe range\n        action = np.clip(action, -1.0, 1.0)\n        \n        # Scale to actual joint limits\n        # (depends on robot configuration)\n        \n        return action\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    controller = RealRobotController(\'walking_policy.pt\')\n    rclpy.spin(controller)\n    controller.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(e.h2,{id:"the-hardware-reality-warning",children:"The Hardware Reality (Warning)"}),"\n",(0,o.jsxs)(e.admonition,{title:"Sim-to-Real Failures",type:"danger",children:[(0,o.jsx)(e.p,{children:"Common failure modes when deploying to real robots:"}),(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Actuator saturation"}),": Real motors can't match simulated torques"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Sensor delays"}),": Real sensors have latency simulation ignores"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Unmodeled dynamics"}),": Cable drag, joint friction, thermal effects"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Environmental differences"}),": Floor texture, lighting, obstacles"]}),"\n"]}),(0,o.jsx)(e.p,{children:"Always start with conservative policies and gradually increase aggressiveness."})]}),"\n",(0,o.jsxs)(e.admonition,{title:"Testing Protocol",type:"warning",children:[(0,o.jsx)(e.p,{children:"Before full deployment:"}),(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Desk test"}),": Run policy with robot suspended"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Harness test"}),": Robot on safety harness"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Slow motion"}),": Run at 50% speed"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Supervised"}),": Human ready to catch/stop"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Full speed"}),": Only after all above pass"]}),"\n"]})]}),"\n",(0,o.jsx)(e.h3,{id:"debugging-sim-to-real-gaps",children:"Debugging Sim-to-Real Gaps"}),"\n",(0,o.jsxs)(e.table,{children:[(0,o.jsx)(e.thead,{children:(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.th,{children:"Symptom"}),(0,o.jsx)(e.th,{children:"Likely Cause"}),(0,o.jsx)(e.th,{children:"Solution"})]})}),(0,o.jsxs)(e.tbody,{children:[(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"Falls immediately"}),(0,o.jsx)(e.td,{children:"Friction mismatch"}),(0,o.jsx)(e.td,{children:"Increase friction randomization"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"Drifts sideways"}),(0,o.jsx)(e.td,{children:"IMU calibration"}),(0,o.jsx)(e.td,{children:"Calibrate real IMU, add bias randomization"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"Jerky motion"}),(0,o.jsx)(e.td,{children:"Action delay"}),(0,o.jsx)(e.td,{children:"Add delay randomization in training"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"Overheats motors"}),(0,o.jsx)(e.td,{children:"Torque limits"}),(0,o.jsx)(e.td,{children:"Add energy penalty, reduce action scale"})]})]})]}),"\n",(0,o.jsx)(e.h2,{id:"assessment",children:"Assessment"}),"\n",(0,o.jsx)(e.h3,{id:"recall",children:"Recall"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"What is the reality gap and why does it exist?"}),"\n",(0,o.jsx)(e.li,{children:"Name three types of domain randomization."}),"\n",(0,o.jsx)(e.li,{children:"What is the purpose of adding observation noise during training?"}),"\n",(0,o.jsx)(e.li,{children:"Why should you start with conservative policies on real robots?"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"apply",children:"Apply"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Implement domain randomization for friction and mass in a simple simulation."}),"\n",(0,o.jsx)(e.li,{children:"Design a reward function for a humanoid robot that encourages stable walking."}),"\n",(0,o.jsx)(e.li,{children:"Write a safety wrapper that limits joint velocities and torques."}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"analyze",children:"Analyze"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Compare the trade-offs between domain randomization and system identification."}),"\n",(0,o.jsx)(e.li,{children:"Why might a policy that works well in simulation with randomization still fail on a real robot?"}),"\n",(0,o.jsx)(e.li,{children:"Design an experiment to identify which simulation parameters most affect real-world performance."}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(c,{...n})}):c(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>r,x:()=>s});var a=i(6540);const o={},t=a.createContext(o);function r(n){const e=a.useContext(t);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:r(n.components),a.createElement(t.Provider,{value:e},n.children)}}}]);