"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[166],{1664:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module4/week13-conversational-robotics","title":"Conversational Robotics & VLA Models","description":"Voice-to-action pipelines, LLM integration, and Vision-Language-Action models for humanoid robots","source":"@site/docs/module4/week13-conversational-robotics.md","sourceDirName":"module4","slug":"/module4/week13-conversational-robotics","permalink":"/physical-ai-and-humanoid-robotics/docs/module4/week13-conversational-robotics","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Conversational Robotics & VLA Models","description":"Voice-to-action pipelines, LLM integration, and Vision-Language-Action models for humanoid robots","keywords":["conversational-ai","whisper","llm","vla","voice-control","robotics"]},"sidebar":"tutorialSidebar","previous":{"title":"Manipulation and Human-Robot Interaction","permalink":"/physical-ai-and-humanoid-robotics/docs/module4/week12-humanoid-dev-2"}}');var s=i(4848),o=i(8453);const a={sidebar_position:3,title:"Conversational Robotics & VLA Models",description:"Voice-to-action pipelines, LLM integration, and Vision-Language-Action models for humanoid robots",keywords:["conversational-ai","whisper","llm","vla","voice-control","robotics"]},r="Conversational Robotics & VLA Models",l={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"The Physics (Why)",id:"the-physics-why",level:2},{value:"The Analogy (Mental Model)",id:"the-analogy-mental-model",level:2},{value:"The Visualization (System Architecture)",id:"the-visualization-system-architecture",level:2},{value:"The Code (Implementation)",id:"the-code-implementation",level:2},{value:"Speech Recognition with Whisper",id:"speech-recognition-with-whisper",level:3},{value:"Cognitive Planning with LLMs",id:"cognitive-planning-with-llms",level:3},{value:"Vision-Language-Action (VLA) Models",id:"vision-language-action-vla-models",level:3},{value:"Architecture",id:"architecture",level:3},{value:"Key VLA Models",id:"key-vla-models",level:3},{value:"The Hardware Reality (Warning)",id:"the-hardware-reality-warning",level:2},{value:"Recommended Hardware Stack",id:"recommended-hardware-stack",level:3},{value:"Capstone Project: The Autonomous Humanoid",id:"capstone-project-the-autonomous-humanoid",level:2},{value:"Assessments",id:"assessments",level:2}];function d(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"conversational-robotics--vla-models",children:"Conversational Robotics & VLA Models"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this chapter, you should be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Integrate speech recognition with robots using OpenAI Whisper"}),"\n",(0,s.jsx)(n.li,{children:"Use LLMs for cognitive planning and task decomposition"}),"\n",(0,s.jsx)(n.li,{children:"Understand Vision-Language-Action (VLA) models"}),"\n",(0,s.jsx)(n.li,{children:"Build a complete voice-to-action pipeline"}),"\n",(0,s.jsx)(n.li,{children:"Design multi-modal interaction systems for humanoid robots"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"the-physics-why",children:"The Physics (Why)"}),"\n",(0,s.jsx)(n.p,{children:"Human communication is inherently multi-modal\u2014we speak, gesture, point, and use facial expressions simultaneously. For robots to work alongside humans naturally, they must understand and respond to these communication channels."}),"\n",(0,s.jsxs)(n.p,{children:["The challenge is bridging the ",(0,s.jsx)(n.strong,{children:"semantic gap"})," between:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Human intent"}),': "Get me a drink from the fridge"']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Robot actions"}),": Navigate \u2192 Open door \u2192 Grasp bottle \u2192 Close door \u2192 Navigate \u2192 Hand over"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This requires understanding language, perceiving the environment, planning actions, and executing them safely\u2014all in real-time."}),"\n",(0,s.jsx)(n.h2,{id:"the-analogy-mental-model",children:"The Analogy (Mental Model)"}),"\n",(0,s.jsxs)(n.p,{children:["Think of a conversational robot as a ",(0,s.jsx)(n.strong,{children:"multilingual translator"})," working in three languages:"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Human Language"}),": Natural speech and gestures"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Semantic Language"}),": Abstract goals and intentions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Robot Language"}),": Motor commands and sensor readings"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Just as a human translator must understand context, culture, and nuance, a conversational robot must understand:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Context"}),': "Put it there" requires knowing what "it" and "there" refer to']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Intent"}),': "Can you get me water?" is a request, not a question about capability']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety"}),': "Throw me the ball" shouldn\'t be interpreted literally for heavy objects']}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"the-visualization-system-architecture",children:"The Visualization (System Architecture)"}),"\n",(0,s.jsx)(n.p,{children:"The future of human-robot interaction lies in natural language. Users should be able to simply tell a robot what to do."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-mermaid",children:"graph LR\n    A[Voice Input] --\x3e B[Whisper ASR]\n    B --\x3e C[LLM Planner]\n    C --\x3e D[Action Sequence]\n    D --\x3e E[ROS 2 Actions]\n    E --\x3e F[Robot Execution]\n"})}),"\n",(0,s.jsx)(n.h2,{id:"the-code-implementation",children:"The Code (Implementation)"}),"\n",(0,s.jsx)(n.h3,{id:"speech-recognition-with-whisper",children:"Speech Recognition with Whisper"}),"\n",(0,s.jsx)(n.p,{children:"OpenAI's Whisper provides state-of-the-art speech recognition:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import whisper\nimport sounddevice as sd\nimport numpy as np\n\n# Load Whisper model\nmodel = whisper.load_model("base")\n\ndef record_audio(duration=5, sample_rate=16000):\n    """Record audio from microphone."""\n    audio = sd.rec(int(duration * sample_rate), \n                   samplerate=sample_rate, \n                   channels=1)\n    sd.wait()\n    return audio.flatten()\n\ndef transcribe(audio):\n    """Transcribe audio to text."""\n    result = model.transcribe(audio)\n    return result["text"]\n\n# Example usage\naudio = record_audio()\ncommand = transcribe(audio)\nprint(f"You said: {command}")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"cognitive-planning-with-llms",children:"Cognitive Planning with LLMs"}),"\n",(0,s.jsx)(n.p,{children:"LLMs can translate natural language commands into robot action sequences:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI()\n\ndef plan_actions(command: str) -> list:\n    """Use GPT to plan robot actions from natural language."""\n    \n    system_prompt = """You are a robot action planner. \n    Convert natural language commands into a sequence of robot actions.\n    Available actions: move_to(x,y), pick_up(object), place(location), \n    rotate(degrees), speak(message)\n    \n    Return actions as a JSON array."""\n    \n    response = client.chat.completions.create(\n        model="gpt-4",\n        messages=[\n            {"role": "system", "content": system_prompt},\n            {"role": "user", "content": command}\n        ]\n    )\n    \n    return response.choices[0].message.content\n\n# Example\ncommand = "Pick up the red cup from the table and bring it to me"\nactions = plan_actions(command)\nprint(actions)\n# Output: [\n#   {"action": "move_to", "params": {"target": "table"}},\n#   {"action": "pick_up", "params": {"object": "red cup"}},\n#   {"action": "move_to", "params": {"target": "user"}},\n#   {"action": "place", "params": {"location": "user_hand"}}\n# ]\n'})}),"\n",(0,s.jsx)(n.h3,{id:"vision-language-action-vla-models",children:"Vision-Language-Action (VLA) Models"}),"\n",(0,s.jsx)(n.p,{children:"VLA models combine vision, language understanding, and action generation in a single model."}),"\n",(0,s.jsx)(n.h3,{id:"architecture",children:"Architecture"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              VLA Model                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Vision    \u2502  Language   \u2502    Action       \u2502\n\u2502   Encoder   \u2502  Encoder    \u2502    Decoder      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Camera    \u2502   Text      \u2502   Motor         \u2502\n\u2502   Input     \u2502   Command   \u2502   Commands      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h3,{id:"key-vla-models",children:"Key VLA Models"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{children:"Developer"}),(0,s.jsx)(n.th,{children:"Capabilities"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"RT-2"}),(0,s.jsx)(n.td,{children:"Google"}),(0,s.jsx)(n.td,{children:"Vision-language-action for manipulation"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"PaLM-E"}),(0,s.jsx)(n.td,{children:"Google"}),(0,s.jsx)(n.td,{children:"Embodied multimodal language model"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Gato"}),(0,s.jsx)(n.td,{children:"DeepMind"}),(0,s.jsx)(n.td,{children:"Generalist agent across tasks"})]})]})]}),"\n",(0,s.jsx)(n.admonition,{title:"The Future",type:"tip",children:(0,s.jsx)(n.p,{children:"VLA models represent the convergence of LLMs and robotics, enabling robots to understand and act on natural language in the physical world."})}),"\n",(0,s.jsx)(n.h2,{id:"the-hardware-reality-warning",children:"The Hardware Reality (Warning)"}),"\n",(0,s.jsxs)(n.admonition,{title:"Latency is Critical",type:"danger",children:[(0,s.jsx)(n.p,{children:"Conversational robots must respond quickly to feel natural:"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Speech recognition"}),": under 500ms for real-time feel"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLM planning"}),": 1-3 seconds acceptable for complex tasks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action execution"}),": Must start within 2 seconds of command"]}),"\n"]}),(0,s.jsx)(n.p,{children:"Running large models locally requires significant hardware:"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Whisper large"}),": 10GB VRAM, ~1s/audio second"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLM (7B params)"}),": 14GB VRAM minimum"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"VLA models"}),": Often require cloud inference"]}),"\n"]})]}),"\n",(0,s.jsxs)(n.admonition,{title:"Edge Deployment Challenges",type:"warning",children:[(0,s.jsx)(n.p,{children:"For standalone humanoid robots:"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Jetson Orin"}),": Can run Whisper small + quantized LLMs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cloud dependency"}),": Larger models require network connectivity"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Privacy concerns"}),": Voice data may contain sensitive information"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reliability"}),": Network failures must have graceful fallbacks"]}),"\n"]})]}),"\n",(0,s.jsx)(n.h3,{id:"recommended-hardware-stack",children:"Recommended Hardware Stack"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Component"}),(0,s.jsx)(n.th,{children:"Edge (Jetson)"}),(0,s.jsx)(n.th,{children:"Workstation"}),(0,s.jsx)(n.th,{children:"Cloud"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"ASR"})}),(0,s.jsx)(n.td,{children:"Whisper tiny/base"}),(0,s.jsx)(n.td,{children:"Whisper medium"}),(0,s.jsx)(n.td,{children:"Whisper large-v3"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"LLM"})}),(0,s.jsx)(n.td,{children:"Llama 3 8B (4-bit)"}),(0,s.jsx)(n.td,{children:"Llama 3 70B"}),(0,s.jsx)(n.td,{children:"GPT-4 / Claude"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"VLA"})}),(0,s.jsx)(n.td,{children:"Not feasible"}),(0,s.jsx)(n.td,{children:"RT-2 (research)"}),(0,s.jsx)(n.td,{children:"API services"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Latency"})}),(0,s.jsx)(n.td,{children:"2-5 seconds"}),(0,s.jsx)(n.td,{children:"1-2 seconds"}),(0,s.jsx)(n.td,{children:"0.5-3 seconds"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"capstone-project-the-autonomous-humanoid",children:"Capstone Project: The Autonomous Humanoid"}),"\n",(0,s.jsx)(n.p,{children:"Your final project integrates everything you've learned:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Voice Command"}),": User speaks a command"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Speech Recognition"}),": Whisper transcribes to text"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Planning"}),": LLM generates action sequence"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Navigation"}),": Nav2 plans path to target"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Perception"}),": Isaac ROS identifies objects"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Manipulation"}),": Robot executes the task"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Capstone architecture\nclass AutonomousHumanoid:\n    def __init__(self):\n        self.whisper = WhisperASR()\n        self.planner = LLMPlanner()\n        self.navigator = Nav2Navigator()\n        self.perception = IsaacPerception()\n        self.manipulator = ArmController()\n    \n    async def execute_command(self, audio):\n        # 1. Transcribe\n        text = self.whisper.transcribe(audio)\n        \n        # 2. Plan\n        actions = self.planner.plan(text)\n        \n        # 3. Execute\n        for action in actions:\n            if action.type == "navigate":\n                await self.navigator.go_to(action.target)\n            elif action.type == "pick":\n                obj = self.perception.find(action.object)\n                await self.manipulator.pick(obj)\n            elif action.type == "place":\n                await self.manipulator.place(action.location)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"assessments",children:"Assessments"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:'Design a voice-to-action pipeline for a robot that can "fetch items from the kitchen."'}),"\n",(0,s.jsx)(n.li,{children:"What are the challenges of using LLMs for real-time robot control?"}),"\n",(0,s.jsx)(n.li,{children:"Compare the approaches of RT-2 and traditional perception + planning pipelines."}),"\n",(0,s.jsx)(n.li,{children:"How would you handle ambiguous voice commands in a conversational robot?"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>r});var t=i(6540);const s={},o=t.createContext(s);function a(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);