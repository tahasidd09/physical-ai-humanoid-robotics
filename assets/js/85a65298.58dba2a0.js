"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[617],{3219:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module3/week9-isaac-2","title":"Isaac ROS and VSLAM","description":"Hardware-accelerated perception with Isaac ROS - Visual SLAM, object detection, and navigation","source":"@site/docs/module3/week9-isaac-2.md","sourceDirName":"module3","slug":"/module3/week9-isaac-2","permalink":"/physical-ai-and-humanoid-robotics/docs/module3/week9-isaac-2","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Isaac ROS and VSLAM","description":"Hardware-accelerated perception with Isaac ROS - Visual SLAM, object detection, and navigation","keywords":["isaac-ros","vslam","perception","navigation","gpu-acceleration"]},"sidebar":"tutorialSidebar","previous":{"title":"NVIDIA Isaac Platform Overview","permalink":"/physical-ai-and-humanoid-robotics/docs/module3/week8-isaac"},"next":{"title":"Sim-to-Real Transfer","permalink":"/physical-ai-and-humanoid-robotics/docs/module3/week10-isaac-3"}}');var s=a(4848),t=a(8453);const r={sidebar_position:2,title:"Isaac ROS and VSLAM",description:"Hardware-accelerated perception with Isaac ROS - Visual SLAM, object detection, and navigation",keywords:["isaac-ros","vslam","perception","navigation","gpu-acceleration"]},o="Isaac ROS and VSLAM",l={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"The Physics (Why)",id:"the-physics-why",level:2},{value:"The Analogy (Mental Model)",id:"the-analogy-mental-model",level:2},{value:"The Visualization (VSLAM Pipeline)",id:"the-visualization-vslam-pipeline",level:2},{value:"The Code (Implementation)",id:"the-code-implementation",level:2},{value:"Stereo Camera Configuration",id:"stereo-camera-configuration",level:3},{value:"Isaac ROS VSLAM Integration",id:"isaac-ros-vslam-integration",level:3},{value:"Launch File for Isaac ROS VSLAM",id:"launch-file-for-isaac-ros-vslam",level:3},{value:"The Hardware Reality (Warning)",id:"the-hardware-reality-warning",level:2},{value:"Handling Tracking Loss",id:"handling-tracking-loss",level:3},{value:"Assessment",id:"assessment",level:2},{value:"Recall",id:"recall",level:3},{value:"Apply",id:"apply",level:3},{value:"Analyze",id:"analyze",level:3}];function d(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"isaac-ros-and-vslam",children:"Isaac ROS and VSLAM"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this chapter, you should be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implement Visual SLAM using Isaac ROS cuVSLAM"}),"\n",(0,s.jsx)(n.li,{children:"Configure stereo cameras for depth perception"}),"\n",(0,s.jsx)(n.li,{children:"Integrate Isaac ROS perception with Nav2 navigation"}),"\n",(0,s.jsx)(n.li,{children:"Optimize perception pipelines for real-time performance"}),"\n",(0,s.jsx)(n.li,{children:"Handle VSLAM tracking failures gracefully"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"the-physics-why",children:"The Physics (Why)"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Visual SLAM"})," (Simultaneous Localization and Mapping) solves a fundamental robotics problem: a robot must build a map of its environment while simultaneously tracking its position within that map."]}),"\n",(0,s.jsx)(n.p,{children:"The physics challenge is that cameras only capture 2D projections of a 3D world. To recover 3D structure, we need:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stereo vision"}),": Two cameras with known separation (baseline)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Triangulation"}),": Calculate depth from pixel disparity"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feature tracking"}),": Match visual features across frames"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Bundle adjustment"}),": Optimize camera poses and 3D points together"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This requires processing millions of pixels per second\u2014exactly what GPUs excel at."}),"\n",(0,s.jsx)(n.h2,{id:"the-analogy-mental-model",children:"The Analogy (Mental Model)"}),"\n",(0,s.jsxs)(n.p,{children:["Think of VSLAM like ",(0,s.jsx)(n.strong,{children:"navigating a new city without GPS"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Observation"}),": You look around and notice landmarks (features)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory"}),": You remember where landmarks are relative to each other (map)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Localization"}),": You recognize landmarks to know where you are"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Exploration"}),": You discover new areas and add them to your mental map"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"A robot does the same thing, but with cameras instead of eyes and algorithms instead of intuition."}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Human Navigation"}),(0,s.jsx)(n.th,{children:"VSLAM Equivalent"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Recognizing landmarks"}),(0,s.jsx)(n.td,{children:"Feature detection (ORB, SIFT)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Estimating distances"}),(0,s.jsx)(n.td,{children:"Stereo depth calculation"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Building mental map"}),(0,s.jsx)(n.td,{children:"3D point cloud construction"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:'Knowing "I\'ve been here"'}),(0,s.jsx)(n.td,{children:"Loop closure detection"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"the-visualization-vslam-pipeline",children:"The Visualization (VSLAM Pipeline)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-mermaid",children:'graph TB\n    subgraph "Sensor Input"\n        A[Left Camera] --\x3e C[Stereo Matcher]\n        B[Right Camera] --\x3e C\n        C --\x3e D[Depth Map]\n    end\n    \n    subgraph "Feature Processing"\n        A --\x3e E[Feature Extraction]\n        E --\x3e F[Feature Matching]\n        F --\x3e G[Motion Estimation]\n    end\n    \n    subgraph "SLAM Backend"\n        D --\x3e H[3D Point Cloud]\n        G --\x3e I[Pose Graph]\n        H --\x3e J[Map]\n        I --\x3e J\n        J --\x3e K[Loop Closure]\n        K --\x3e I\n    end\n    \n    subgraph "Output"\n        I --\x3e L[/visual_slam/odometry]\n        J --\x3e M[/visual_slam/map]\n    end\n'})}),"\n",(0,s.jsx)(n.h2,{id:"the-code-implementation",children:"The Code (Implementation)"}),"\n",(0,s.jsx)(n.h3,{id:"stereo-camera-configuration",children:"Stereo Camera Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nstereo_camera_config.py - Configure stereo cameras for Isaac ROS VSLAM.\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import CameraInfo, Image\nimport numpy as np\n\n\nclass StereoCameraPublisher(Node):\n    """Publishes stereo camera info for VSLAM."""\n    \n    def __init__(self):\n        super().__init__(\'stereo_camera_publisher\')\n        \n        # Camera parameters (Intel RealSense D435i style)\n        self.image_width = 640\n        self.image_height = 480\n        self.baseline = 0.05  # 50mm baseline\n        self.focal_length = 380.0  # pixels\n        \n        # Publishers for camera info\n        self.left_info_pub = self.create_publisher(\n            CameraInfo, \'/camera/left/camera_info\', 10\n        )\n        self.right_info_pub = self.create_publisher(\n            CameraInfo, \'/camera/right/camera_info\', 10\n        )\n        \n        # Publish camera info at 30 Hz\n        self.timer = self.create_timer(1/30, self.publish_camera_info)\n        \n        self.get_logger().info(\'Stereo camera publisher started\')\n    \n    def create_camera_info(self, is_left: bool) -> CameraInfo:\n        """Create CameraInfo message for stereo camera."""\n        info = CameraInfo()\n        info.header.stamp = self.get_clock().now().to_msg()\n        info.header.frame_id = \'camera_left\' if is_left else \'camera_right\'\n        \n        info.width = self.image_width\n        info.height = self.image_height\n        \n        # Intrinsic matrix K\n        cx = self.image_width / 2\n        cy = self.image_height / 2\n        info.k = [\n            self.focal_length, 0.0, cx,\n            0.0, self.focal_length, cy,\n            0.0, 0.0, 1.0\n        ]\n        \n        # Projection matrix P\n        # For right camera, Tx = -fx * baseline\n        tx = 0.0 if is_left else -self.focal_length * self.baseline\n        info.p = [\n            self.focal_length, 0.0, cx, tx,\n            0.0, self.focal_length, cy, 0.0,\n            0.0, 0.0, 1.0, 0.0\n        ]\n        \n        # Rectification matrix R (identity for rectified images)\n        info.r = [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]\n        \n        # Distortion (assuming rectified images)\n        info.distortion_model = \'plumb_bob\'\n        info.d = [0.0, 0.0, 0.0, 0.0, 0.0]\n        \n        return info\n    \n    def publish_camera_info(self):\n        """Publish camera info for both cameras."""\n        self.left_info_pub.publish(self.create_camera_info(is_left=True))\n        self.right_info_pub.publish(self.create_camera_info(is_left=False))\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = StereoCameraPublisher()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"isaac-ros-vslam-integration",children:"Isaac ROS VSLAM Integration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nvslam_integration.py - Integrate Isaac ROS VSLAM with robot control.\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom nav_msgs.msg import Odometry, Path\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom std_msgs.msg import Bool\nimport numpy as np\nfrom enum import Enum\n\n\nclass TrackingState(Enum):\n    """VSLAM tracking states."""\n    INITIALIZING = 0\n    TRACKING = 1\n    LOST = 2\n    RELOCALIZING = 3\n\n\nclass VSLAMNavigator(Node):\n    """Navigate using Isaac ROS VSLAM for localization."""\n    \n    def __init__(self):\n        super().__init__(\'vslam_navigator\')\n        \n        # VSLAM state\n        self.tracking_state = TrackingState.INITIALIZING\n        self.current_pose = None\n        self.pose_covariance = None\n        self.path_history = []\n        \n        # Subscribers\n        self.odom_sub = self.create_subscription(\n            Odometry,\n            \'/visual_slam/tracking/odometry\',\n            self.odom_callback,\n            10\n        )\n        \n        self.tracking_sub = self.create_subscription(\n            Bool,\n            \'/visual_slam/status/tracking\',\n            self.tracking_callback,\n            10\n        )\n        \n        # Publishers\n        self.cmd_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.path_pub = self.create_publisher(Path, \'/robot_path\', 10)\n        \n        # Safety parameters\n        self.max_covariance = 0.1  # Stop if localization uncertain\n        self.lost_timeout = 2.0  # Seconds before declaring lost\n        self.last_good_tracking = self.get_clock().now()\n        \n        # Control loop\n        self.timer = self.create_timer(0.1, self.control_loop)\n        \n        self.get_logger().info(\'VSLAM Navigator started\')\n    \n    def odom_callback(self, msg: Odometry):\n        """Process VSLAM odometry updates."""\n        self.current_pose = msg.pose.pose\n        \n        # Extract position covariance (diagonal elements)\n        cov = msg.pose.covariance\n        self.pose_covariance = np.sqrt(cov[0]**2 + cov[7]**2 + cov[14]**2)\n        \n        # Update path history\n        pose_stamped = PoseStamped()\n        pose_stamped.header = msg.header\n        pose_stamped.pose = msg.pose.pose\n        self.path_history.append(pose_stamped)\n        \n        # Limit path history length\n        if len(self.path_history) > 1000:\n            self.path_history = self.path_history[-500:]\n        \n        # Publish path for visualization\n        path_msg = Path()\n        path_msg.header = msg.header\n        path_msg.poses = self.path_history\n        self.path_pub.publish(path_msg)\n        \n        # Update tracking state\n        if self.pose_covariance < self.max_covariance:\n            self.tracking_state = TrackingState.TRACKING\n            self.last_good_tracking = self.get_clock().now()\n        else:\n            self.tracking_state = TrackingState.RELOCALIZING\n    \n    def tracking_callback(self, msg: Bool):\n        """Handle tracking status updates."""\n        if not msg.data:\n            elapsed = (self.get_clock().now() - self.last_good_tracking).nanoseconds / 1e9\n            if elapsed > self.lost_timeout:\n                self.tracking_state = TrackingState.LOST\n                self.get_logger().error(\'VSLAM tracking lost!\')\n    \n    def control_loop(self):\n        """Main control loop with safety checks."""\n        cmd = Twist()\n        \n        if self.tracking_state == TrackingState.LOST:\n            # Emergency stop when tracking is lost\n            cmd.linear.x = 0.0\n            cmd.angular.z = 0.0\n            self.get_logger().warn(\'Stopped: VSLAM tracking lost\')\n            \n        elif self.tracking_state == TrackingState.RELOCALIZING:\n            # Slow rotation to help relocalization\n            cmd.linear.x = 0.0\n            cmd.angular.z = 0.2  # Slow rotation\n            self.get_logger().info(\'Relocalizing...\')\n            \n        elif self.tracking_state == TrackingState.TRACKING:\n            # Normal operation - navigation commands would go here\n            pass\n        \n        self.cmd_pub.publish(cmd)\n    \n    def get_position(self):\n        """Get current position if tracking is good."""\n        if self.tracking_state == TrackingState.TRACKING and self.current_pose:\n            return (\n                self.current_pose.position.x,\n                self.current_pose.position.y,\n                self.current_pose.position.z\n            )\n        return None\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VSLAMNavigator()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"launch-file-for-isaac-ros-vslam",children:"Launch File for Isaac ROS VSLAM"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nisaac_vslam_launch.py - Launch Isaac ROS VSLAM with configuration.\n\"\"\"\n\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\n\n\ndef generate_launch_description():\n    \"\"\"Generate launch description for Isaac ROS VSLAM.\"\"\"\n    \n    return LaunchDescription([\n        # Declare arguments\n        DeclareLaunchArgument(\n            'enable_imu_fusion',\n            default_value='true',\n            description='Enable IMU fusion for better tracking'\n        ),\n        \n        DeclareLaunchArgument(\n            'enable_slam',\n            default_value='true',\n            description='Enable mapping (vs localization only)'\n        ),\n        \n        # Isaac ROS VSLAM node\n        Node(\n            package='isaac_ros_visual_slam',\n            executable='visual_slam_node',\n            name='visual_slam',\n            parameters=[{\n                'enable_imu_fusion': LaunchConfiguration('enable_imu_fusion'),\n                'enable_slam': LaunchConfiguration('enable_slam'),\n                'rectified_images': True,\n                'enable_observations_view': True,\n                'enable_landmarks_view': True,\n                'map_frame': 'map',\n                'odom_frame': 'odom',\n                'base_frame': 'base_link',\n            }],\n            remappings=[\n                ('stereo_camera/left/image', '/camera/left/image_raw'),\n                ('stereo_camera/left/camera_info', '/camera/left/camera_info'),\n                ('stereo_camera/right/image', '/camera/right/image_raw'),\n                ('stereo_camera/right/camera_info', '/camera/right/camera_info'),\n                ('visual_slam/imu', '/imu'),\n            ]\n        ),\n        \n        # VSLAM Navigator node\n        Node(\n            package='humanoid_navigation',\n            executable='vslam_navigator',\n            name='vslam_navigator',\n            output='screen'\n        ),\n    ])\n"})}),"\n",(0,s.jsx)(n.h2,{id:"the-hardware-reality-warning",children:"The Hardware Reality (Warning)"}),"\n",(0,s.jsxs)(n.admonition,{title:"VSLAM Failure Modes",type:"danger",children:[(0,s.jsx)(n.p,{children:"Visual SLAM can fail in challenging conditions:"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Low light"}),": Insufficient features to track"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fast motion"}),": Motion blur destroys features"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Textureless surfaces"}),": White walls, blank floors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Dynamic scenes"}),": Moving objects confuse tracking"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Repetitive patterns"}),": Causes incorrect loop closures"]}),"\n"]}),(0,s.jsx)(n.p,{children:"Always implement fallback localization (wheel odometry, IMU dead reckoning)."})]}),"\n",(0,s.jsxs)(n.admonition,{title:"Performance Tuning",type:"warning",children:[(0,s.jsx)(n.p,{children:"Isaac ROS VSLAM performance depends on configuration:"}),(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Parameter"}),(0,s.jsx)(n.th,{children:"Low Power"}),(0,s.jsx)(n.th,{children:"Balanced"}),(0,s.jsx)(n.th,{children:"High Accuracy"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Image resolution"}),(0,s.jsx)(n.td,{children:"320x240"}),(0,s.jsx)(n.td,{children:"640x480"}),(0,s.jsx)(n.td,{children:"1280x720"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Feature count"}),(0,s.jsx)(n.td,{children:"500"}),(0,s.jsx)(n.td,{children:"1000"}),(0,s.jsx)(n.td,{children:"2000"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Keyframe rate"}),(0,s.jsx)(n.td,{children:"2 Hz"}),(0,s.jsx)(n.td,{children:"5 Hz"}),(0,s.jsx)(n.td,{children:"10 Hz"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"GPU memory"}),(0,s.jsx)(n.td,{children:"1 GB"}),(0,s.jsx)(n.td,{children:"2 GB"}),(0,s.jsx)(n.td,{children:"4 GB"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Latency"}),(0,s.jsx)(n.td,{children:"20ms"}),(0,s.jsx)(n.td,{children:"35ms"}),(0,s.jsx)(n.td,{children:"60ms"})]})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"handling-tracking-loss",children:"Handling Tracking Loss"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class TrackingRecovery:\n    """Strategies for recovering from VSLAM tracking loss."""\n    \n    def __init__(self):\n        self.recovery_strategies = [\n            self.slow_rotation,\n            self.return_to_last_known,\n            self.wheel_odometry_fallback,\n        ]\n    \n    def slow_rotation(self):\n        """Rotate slowly to find recognizable features."""\n        # Rotate 360 degrees slowly\n        pass\n    \n    def return_to_last_known(self):\n        """Navigate back to last known good position."""\n        # Use wheel odometry to return\n        pass\n    \n    def wheel_odometry_fallback(self):\n        """Fall back to wheel odometry until VSLAM recovers."""\n        # Switch to odometry-only navigation\n        pass\n'})}),"\n",(0,s.jsx)(n.h2,{id:"assessment",children:"Assessment"}),"\n",(0,s.jsx)(n.h3,{id:"recall",children:"Recall"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"What does VSLAM stand for and what problem does it solve?"}),"\n",(0,s.jsx)(n.li,{children:"Why is stereo vision necessary for depth perception in VSLAM?"}),"\n",(0,s.jsx)(n.li,{children:"What is loop closure and why is it important?"}),"\n",(0,s.jsx)(n.li,{children:"List three conditions that can cause VSLAM tracking to fail."}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"apply",children:"Apply"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Configure Isaac ROS VSLAM for a robot with a 10cm stereo baseline."}),"\n",(0,s.jsx)(n.li,{children:"Write a node that detects VSLAM tracking loss and triggers an emergency stop."}),"\n",(0,s.jsx)(n.li,{children:"Implement a simple recovery behavior that rotates the robot when tracking is lost."}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"analyze",children:"Analyze"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Compare the trade-offs between VSLAM and LiDAR-based SLAM for indoor navigation."}),"\n",(0,s.jsx)(n.li,{children:"Why might a humanoid robot need different VSLAM parameters when walking vs. standing still?"}),"\n",(0,s.jsx)(n.li,{children:"Design a sensor fusion strategy that combines VSLAM with wheel odometry and IMU for robust localization."}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>r,x:()=>o});var i=a(6540);const s={},t=i.createContext(s);function r(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);